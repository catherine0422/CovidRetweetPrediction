{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr.models import xDeepFM, DeepFM\n",
    "from deepctr.feature_column import  SparseFeat, DenseFeat, get_feature_names\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>ratio_friends_followers</th>\n",
       "      <th>mention_exist</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_exist</th>\n",
       "      <th>url_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_0</th>\n",
       "      <th>tf_idf_1</th>\n",
       "      <th>tf_idf_2</th>\n",
       "      <th>tf_idf_3</th>\n",
       "      <th>tf_idf_4</th>\n",
       "      <th>tf_idf_5</th>\n",
       "      <th>tf_idf_6</th>\n",
       "      <th>tf_idf_7</th>\n",
       "      <th>tf_idf_8</th>\n",
       "      <th>tf_idf_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68460</td>\n",
       "      <td>1101</td>\n",
       "      <td>1226</td>\n",
       "      <td>1.112523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.606137e-15</td>\n",
       "      <td>-1.992248e-13</td>\n",
       "      <td>-6.077335e-13</td>\n",
       "      <td>-1.726278e-12</td>\n",
       "      <td>7.402787e-13</td>\n",
       "      <td>2.375980e-14</td>\n",
       "      <td>2.692723e-13</td>\n",
       "      <td>5.657295e-13</td>\n",
       "      <td>-2.124336e-14</td>\n",
       "      <td>9.444088e-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  user_verified  user_statuses_count  user_followers_count  \\\n",
       "0   0              0                68460                  1101   \n",
       "\n",
       "   user_friends_count  ratio_friends_followers  mention_exist  mention_count  \\\n",
       "0                1226                 1.112523              0              0   \n",
       "\n",
       "   url_exist  url_count  ...      tf_idf_0      tf_idf_1      tf_idf_2  \\\n",
       "0          0          0  ... -1.606137e-15 -1.992248e-13 -6.077335e-13   \n",
       "\n",
       "       tf_idf_3      tf_idf_4      tf_idf_5      tf_idf_6      tf_idf_7  \\\n",
       "0 -1.726278e-12  7.402787e-13  2.375980e-14  2.692723e-13  5.657295e-13   \n",
       "\n",
       "       tf_idf_8      tf_idf_9  \n",
       "0 -2.124336e-14  9.444088e-14  \n",
       "\n",
       "[1 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(train_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['retweet_count']  #series\n",
    "y_test = test['retweet_count']    #series\n",
    "X_train = train.drop(['retweet_count','id'], axis=1)   #dataframe\n",
    "X_test = test.drop(['retweet_count','id'], axis=1)     #dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the evaluation metric  计算MAE分数\n",
    "# the inputs need to be np.darray\n",
    "\n",
    "def MAE(y_predict, y_true):\n",
    "    n = len(y_predict)\n",
    "    res = 0\n",
    "    for i in range(n):\n",
    "        res = res + abs(y_predict[i]-y_true[i])\n",
    "    return res/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lrrf(X_train_lr, y_train_lr):\n",
    "    \"\"\" Train and store LRRF (LR + Random Forest) \"\"\"\n",
    "    reg = LinearRegression(fit_intercept=False).fit(X_train_lr.values, y_train_lr)\n",
    "    filename = './model/lr_demo.sav'  #每次改参数后文件名也要改，方便查询\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "    lr_y_train_predict = reg.predict(X_train_lr.values) # this produces a ndarray\n",
    "    print(\"The training MAE for linear regression is {}\".format(MAE(lr_y_train_predict, np.array(y_train_lr))))\n",
    "    \n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: LinearRegressor(fit_intercept=False) \\n\\n\".format(filename) #这里的参数也要改，方便查询日志\n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) \n",
    "    \n",
    "    # 随机森林部分\n",
    "    # for training residual RF\n",
    "    rf_y_train = y_train_lr - lr_y_train_predict\n",
    "    \n",
    "    \n",
    "    reg = RandomForestRegressor(max_depth = 20,   #参数\n",
    "                                n_estimators = 500, #参数\n",
    "                                random_state = 7,  #参数\n",
    "                                n_jobs = 10, #同时进行任务的个数，论文里为3\n",
    "                                verbose = 5)  \n",
    "    start_time = time.time()\n",
    "    reg.fit(X_train_lr.values, rf_y_train, )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"took {} seconds for fitting\".format(elapsed_time))\n",
    "    \n",
    "    # save randomforest regressor\n",
    "    filename = './model/lrrf_demo.sav'  #每次改参数后文件名也要改，方便查询\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "    \n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \" {}: Random forest regressor: max_depth=20, n_estimators=500, random_state=7 \\n\\n\".format(filename)  #这里的参数也要改，方便查询日志\n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nnrf(X_train_nn, y_train_nn):\n",
    "    \"\"\" Train and store NNRF (Neural Networks - MLP + Random Forest) \"\"\"\n",
    "    \n",
    "    ########### regression\n",
    "    reg = MLPRegressor(random_state=7,  # 参数\n",
    "                        hidden_layer_sizes=(64,32,16,8,8),  # 参数\n",
    "                        batch_size=1024,  # 参数\n",
    "                        learning_rate_init=.01,\n",
    "                        early_stopping=False,\n",
    "                        verbose=True,\n",
    "                        shuffle=True,\n",
    "                        n_iter_no_change=10)\n",
    "    start_time = time.time()\n",
    "    reg.fit(X_train_nn.values, y_train_nn)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"took {} seconds for fitting\".format(elapsed_time))\n",
    "    \n",
    "    filename = './model/nn_demo10.sav' #每次改参数后文件名也要改，方便查询\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "    \n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: MLPRegressor(batch_size=4096,hidden_layer_sizes=(128,64,32,8),\\\n",
    "    random_state=211) \\n\\n\".format(filename) #这里的参数也要改，方便查询日志\n",
    "    \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) \n",
    "        \n",
    "    ############ Random Forest\n",
    "    nn_y_train_predict = reg.predict(X_train_nn.values)\n",
    "    print(\"The training MAE for neural networks is {}\".format(MAE(nn_y_train_predict, np.array(y_train_nn))))\n",
    "    \n",
    "    # for training residual RF\n",
    "    rf_y_train = y_train_nn - nn_y_train_predict\n",
    "    \n",
    "    reg = RandomForestRegressor(max_depth=18, # 参数\n",
    "                                n_estimators=500, # 参数\n",
    "                                random_state=7, # 参数\n",
    "                                n_jobs=10, #同时进行任务的个数，论文里为3\n",
    "                                verbose=5)\n",
    "    start_time = time.time()\n",
    "    reg.fit(X_train_nn.values, rf_y_train, )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"took {} seconds for fitting\".format(elapsed_time))\n",
    "    \n",
    "    # save randomforest regressor \n",
    "    filename = './model/nnrf_demo10.sav'  #每次改参数后文件名也要改，方便查询\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "    \n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: RandomForestRegressor(n_estimators=500, max_depth=18, random_state=211) \\n\\n\".format(filename)  \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XdeepFMRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xdeepfmrf(X_train, y_train):\n",
    "    \"\"\" Train and store FMRF (xDeepFM + Random Forest) \"\"\"\n",
    "    features = X_train.columns.values.tolist()\n",
    "    sparse_features = [\"timeseg\", \"day_of_week\"] # 如果训练集里没有这两个变量则sparse_features = []\n",
    "    dense_features = [f for f in features if f not in [\"timeseg\", \"day_of_week\"]]\n",
    "    \n",
    "    def encoding(data, feat, encoder):\n",
    "        data[feat] = encoder.fit_transform(data[feat])\n",
    "    \n",
    "    [encoding(X_train, feat, LabelEncoder()) for feat in sparse_features]\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=X_train[feat].nunique(), embedding_dim=4) \\\n",
    "                              for i, feat in enumerate(sparse_features)]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "    \n",
    "    # features to be used for dnn part of xdeepfm\n",
    "    dnn_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "    # features to be used for linear part of xdeepfm\n",
    "    linear_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    train_model_input = {name: X_train[name].values for name in feature_names}\n",
    "    \n",
    "    model = xDeepFM(linear_feature_columns, dnn_feature_columns, task='regression', seed=28)\n",
    "    # compiling the model\n",
    "    model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "    # training the model\n",
    "    history = model.fit(train_model_input, y_train, batch_size=256, epochs=20, verbose=2)\n",
    "    filename = './model/xDeepFM_w.h5'\n",
    "    model.save_weights(filename) \n",
    "\n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: xdeepfm weights (batch_size=256, epochs=20, verbose=2) \\n\\n\".format(filename)  \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) \n",
    "    \n",
    "    # random forest\n",
    "    pred = model.predict(train_model_input, batch_size=256)\n",
    "    rf_y_train = y_train - pred.reshape(X_train.shape[0], )\n",
    "    \n",
    "    reg = RandomForestRegressor(max_depth=16,\n",
    "                                max_features=.5,\n",
    "                                n_estimators=500,\n",
    "                                random_state=28,\n",
    "                                n_jobs=10,\n",
    "                                verbose=5)\n",
    "    start_time = time.time()\n",
    "    reg.fit(X_train[features].values, rf_y_train, )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"took {} seconds for fitting\".format(elapsed_time))\n",
    "    filename = './model/randomforest_regressor_500e_xdeepfm_rs211_28.sav'\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "    \n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: Random forest for xdeepfm (n_estimators=500, max_depth=16, random_state=28) \\n\\n\".format(filename)  \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFMRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deepfmrf(X_train, y_train):\n",
    "    \"\"\" Train and store FMRF (DeepFM + Random Forest) \"\"\"\n",
    "    \n",
    "    features = X_train.columns.values.tolist()\n",
    "    sparse_features = [\"timeseg\", \"day_of_week\"] # 如果训练集里没有这两个变量则sparse_features = []\n",
    "    dense_features = [f for f in features if f not in [\"timeseg\", \"day_of_week\"]]\n",
    "    \n",
    "    def encoding(data, feat, encoder):\n",
    "        data[feat] = encoder.fit_transform(data[feat])\n",
    "\n",
    "    [encoding(X_train, feat, LabelEncoder()) for feat in sparse_features]\n",
    "    \n",
    "\n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=X_train[feat].nunique(), embedding_dim=4) \\\n",
    "                              for i, feat in enumerate(sparse_features)]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "    \n",
    "    # features to be used for dnn part of xdeepfm\n",
    "    dnn_feature_columns = dense_feature_columns\n",
    "    # features to be used for linear part of xdeepfm\n",
    "    linear_feature_columns = dense_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    train_model_input = {name: X_train[name].values for name in feature_names}\n",
    "    \n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression', seed=29)\n",
    "    \n",
    "    # compiling the model\n",
    "    model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "    # training the model\n",
    "    history = model.fit(train_model_input, y_train, batch_size=4096, epochs=150, verbose=2)\n",
    "    filename = './model/xDeepFM_w_seed29.h5'\n",
    "    model.save_weights(filename)\n",
    "\n",
    "\n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: deepfm weights (batch_size=4096, epochs=150, verbose=2) \\n\\n\".format(filename)  \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) \n",
    "    \n",
    "    \n",
    "    pred = model.predict(train_model_input, batch_size=4096)\n",
    "    rf_y_train = y_train - pred.reshape(X_train.shape[0], )\n",
    "    \n",
    "    # random forest\n",
    "    reg = RandomForestRegressor(max_depth=16,\n",
    "                                max_features=.5,\n",
    "                                n_estimators=500,\n",
    "                                random_state=29,\n",
    "                                n_jobs=10,\n",
    "                                verbose=5)\n",
    "    start_time = time.time()\n",
    "    reg.fit(X_train[features].values, rf_y_train, )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"took {} seconds for fitting\".format(elapsed_time))\n",
    "    filename = './model/randomforest_regressor_500e_xdeepfm_rs211_29.sav'\n",
    "    pickle.dump(reg, open(filename, 'wb'))\n",
    "\n",
    "    # update the log file\n",
    "    logname = 'log.txt'\n",
    "    str = \"{}: Random forest for deepfm (n_estimators=500, max_depth=16, random_state=29) \\n\\n\".format(filename)  \n",
    "    with open(logname, 'a+') as f:\n",
    "        f.write(str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrrf_predict(X_test_lr, y_test_lr):\n",
    "\n",
    "    # model 1 - LRRF\n",
    "    # load lr\n",
    "    filename = './model/lr_demo.sav'\n",
    "    reg = pickle.load(open(filename, 'rb'))\n",
    "    lr_y_predict = reg.predict(X_test_lr.values)\n",
    "\n",
    "    # load rf\n",
    "    filename = './model/lrrf_demo.sav'\n",
    "    reg = pickle.load(open(filename, 'rb'))\n",
    "    rf_predict = reg.predict(X_test_lr.values)\n",
    "\n",
    "    result_list = list()\n",
    "    for e in reg.estimators_:\n",
    "        result_list.append(e.predict(X_test_lr.values))\n",
    "\n",
    "    result_list = np.array(result_list)\n",
    "    print(result_list.shape)\n",
    "\n",
    "    # combine\n",
    "    y_predict = lr_y_predict + rf_predict\n",
    "    print(\"The testing MAE for lrrf is {}\".format(MAE(y_predict, np.array(y_test_lr))))\n",
    "    # save results\n",
    "    np.savetxt(\"output/model1.predict\", y_predict.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnrf_predict(X_test_nn, y_test_nn):\n",
    "    \n",
    "    model_dict = {\n",
    "        './model/nn_demo1.sav': './model/nnrf_demo1.sav', # model 2\n",
    "        './model/nn_demo2.sav': './model/nnrf_demo2.sav', # model 3\n",
    "        './model/nn_demo3.sav': './model/nnrf_demo3.sav', # model 4\n",
    "        './model/nn_demo4.sav': './model/nnrf_demo4.sav', # model 5\n",
    "        './model/nn_demo5.sav': './model/nnrf_demo5.sav', # model 6\n",
    "        './model/nn_demo6.sav': './model/nnrf_demo6.sav', # model 7\n",
    "        './model/nn_demo7.sav': './model/nnrf_demo7.sav', # model 8\n",
    "        './model/nn_demo8.sav': './model/nnrf_demo8.sav',\n",
    "        './model/nn_demo9.sav': './model/nnrf_demo9.sav',\n",
    "        './model/nn_demo10.sav': './model/nnrf_demo10.sav'\n",
    "    }\n",
    "    \n",
    "    for idx, regressor_path in enumerate(model_dict.keys()):\n",
    "        regr = pickle.load(open(regressor_path, 'rb'))\n",
    "        nn_y_val_predict = regr.predict(X_test_nn.values)\n",
    "\n",
    "        # load rf\n",
    "        filename = model_dict[regressor_path]\n",
    "        reg = pickle.load(open(filename, 'rb'))\n",
    "        rf_val_predict = reg.predict(X_test_nn.values)\n",
    "\n",
    "        # combine\n",
    "        y_val_predict = nn_y_val_predict + rf_val_predict\n",
    "        np.savetxt(\"output/model{}.predict\".format(idx+2), y_val_predict.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FMRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fmrf_predict(val):\n",
    "    \n",
    "    ############ Model xDeepFM\n",
    "    \n",
    "    # load rf\n",
    "    filename = './model/randomforest_regressor_500e_xdeepfm_rs211_28.sav'\n",
    "    reg = pickle.load(open(filename, 'rb'))\n",
    "    rf_val_predict = reg.predict(X_test.values)\n",
    "    \n",
    "    # DeepFM\n",
    "    features = val.columns.values.tolist()\n",
    "    sparse_features = [\"timeseg\", \"day_of_week\"]  # 如果训练集里没有这两个变量则sparse_features = []\n",
    "    dense_features = [f for f in features if f not in [\"timeseg\", \"day_of_week\"]]\n",
    "    \n",
    "    def encoding(data, feat, encoder):\n",
    "        data[feat] = encoder.fit_transform(data[feat])\n",
    "\n",
    "    [encoding(val, feat, LabelEncoder()) for feat in sparse_features]\n",
    "    \n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=val[feat].nunique(), embedding_dim=4) \\\n",
    "                              for i, feat in enumerate(sparse_features)]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "    print(len(dense_feature_columns))\n",
    "    \n",
    "    # features to be used for dnn part of xdeepfm\n",
    "    dnn_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "    # features to be used for linear part of xdeepfm\n",
    "    linear_feature_columns = sparse_feature_columns + dense_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    model = xDeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "    model.load_weights('./model/xDeepFM_w.h5')\n",
    "    \n",
    "    test_model_input = {name: val[name].values for name in feature_names}\n",
    "\n",
    "    deepfm_pred = model.predict(test_model_input, batch_size=256)\n",
    "    deepfm_pred = deepfm_pred.reshape(rf_val_predict.shape)\n",
    "    deepfm_pred_counter = deepfm_pred + rf_val_predict\n",
    "    \n",
    "    np.savetxt(\"output/model12.predict\", deepfm_pred_counter.astype(int), fmt='%i')\n",
    "    \n",
    "    ############   Model DeepFM\n",
    "    \n",
    "    # RF\n",
    "    # load rf\n",
    "    filename = './model/randomforest_regressor_500e_xdeepfm_rs211_29.sav'\n",
    "    reg = pickle.load(open(filename, 'rb'))\n",
    "    rf_val_predict = reg.predict(val[features].values)\n",
    "\n",
    "    ###########################\n",
    "    # DeepFM\n",
    "\n",
    "    sparse_features = [\"timeseg\", \"day_of_week\"]  # 如果训练集里没有这两个变量则sparse_features = []\n",
    "    dense_features = [f for f in features if f not in [\"timeseg\", \"day_of_week\"]]\n",
    "    \n",
    "    def encoding(data, feat, encoder):\n",
    "        data[feat] = encoder.fit_transform(data[feat])\n",
    "\n",
    "    [encoding(val, feat, LabelEncoder()) for feat in sparse_features]\n",
    "\n",
    "    sparse_feature_columns = [SparseFeat(feat, vocabulary_size=val[feat].nunique(), embedding_dim=4) \\\n",
    "                              for i, feat in enumerate(sparse_features)]\n",
    "    dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "    print(len(dense_feature_columns))\n",
    "    \n",
    "    # features to be used for dnn part of xdeepfm\n",
    "    dnn_feature_columns = dense_feature_columns\n",
    "    # features to be used for linear part of xdeepfm\n",
    "    linear_feature_columns = dense_feature_columns\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "    \n",
    "    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "    model.load_weights('./model/xDeepFM_w_seed29.h5')\n",
    "    \n",
    "    test_model_input = {name: val[name].values for name in feature_names}\n",
    "\n",
    "    deepfm_pred = model.predict(test_model_input, batch_size=256)\n",
    "    deepfm_pred = deepfm_pred.reshape(rf_val_predict.shape)\n",
    "    deepfm_pred_counter = deepfm_pred + rf_val_predict\n",
    "    np.savetxt(\"output/model13.predict\", deepfm_pred_counter.astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_model_predict(val):\n",
    "    if not os.path.exists('output'):\n",
    "        os.makedirs('output')\n",
    "        print('created output folder as it is not existing...')\n",
    "        \n",
    "    if not os.path.exists('output/model1.predict'):\n",
    "        lrrf_predict(val)\n",
    "    if not os.path.exists('output/model2.predict'):\n",
    "        nnrf_predict(val)\n",
    "    if not os.path.exists('output/model12.predict'):\n",
    "        fmrf_predict(val)\n",
    "        \n",
    "    # load separate prediction files\n",
    "    predict = pd.read_csv('output/model1.predict', header=None)\n",
    "    predict.columns = [\"model1\"]\n",
    "\n",
    "    model2_predict = pd.read_csv('output/model2.predict', header=None)\n",
    "    model2_predict.columns = [\"yhat\"]\n",
    "    model3_predict = pd.read_csv('output/model3.predict', header=None)\n",
    "    model3_predict.columns = [\"yhat\"]\n",
    "    model4_predict = pd.read_csv('output/model4.predict', header=None)\n",
    "    model4_predict.columns = [\"yhat\"]\n",
    "    model5_predict = pd.read_csv('output/model5.predict', header=None)\n",
    "    model5_predict.columns = [\"yhat\"]\n",
    "    model6_predict = pd.read_csv('output/model6.predict', header=None)\n",
    "    model6_predict.columns = [\"yhat\"]\n",
    "    model7_predict = pd.read_csv('output/model7.predict', header=None)\n",
    "    model7_predict.columns = [\"yhat\"]\n",
    "    model8_predict = pd.read_csv('output/model8.predict', header=None)\n",
    "    model8_predict.columns = [\"yhat\"]\n",
    "    model9_predict = pd.read_csv('output/model9.predict', header=None)\n",
    "    model9_predict.columns = [\"yhat\"]\n",
    "    model10_predict = pd.read_csv('output/model10.predict', header=None)\n",
    "    model10_predict.columns = [\"yhat\"]\n",
    "    model11_predict = pd.read_csv('output/model11.predict', header=None)\n",
    "    model11_predict.columns = [\"yhat\"]\n",
    "    model12_predict = pd.read_csv('output/model12.predict', header=None)\n",
    "    model12_predict.columns = [\"yhat\"]\n",
    "    model13_predict = pd.read_csv('output/model13.predict', header=None)\n",
    "    model13_predict.columns = [\"yhat\"]\n",
    "    model14_predict = pd.read_csv('output/model14.predict', header=None)\n",
    "    model14_predict.columns = [\"yhat\"]\n",
    "\n",
    "    # arithmetic mean\n",
    "    predict[\"model2\"] = model2_predict[\"yhat\"]\n",
    "    predict[\"model3\"] = model3_predict[\"yhat\"]\n",
    "    predict[\"model4\"] = model4_predict[\"yhat\"]\n",
    "    predict[\"model5\"] = model5_predict[\"yhat\"]\n",
    "    predict[\"model6\"] = model6_predict[\"yhat\"]\n",
    "    predict[\"model7\"] = model7_predict[\"yhat\"]\n",
    "    predict[\"model8\"] = model8_predict[\"yhat\"] * 2\n",
    "    predict[\"model9\"] = model9_predict[\"yhat\"]\n",
    "    predict[\"model10\"] = model10_predict[\"yhat\"]\n",
    "    predict[\"model11\"] = model11_predict[\"yhat\"]\n",
    "    predict[\"model12\"] = model12_predict[\"yhat\"]\n",
    "    predict[\"model13\"] = model13_predict[\"yhat\"] * 2\n",
    "    predict[\"model14\"] = model14_predict[\"yhat\"]\n",
    "    predict[\"yhatavg\"] = (\n",
    "                            predict[\"model1\"] + predict[\"model2\"] \\\n",
    "                            + predict[\"model3\"] + predict[\"model4\"] \\\n",
    "                            + predict[\"model5\"] + predict[\"model6\"] \\\n",
    "                            + predict[\"model7\"] + predict[\"model8\"] \\\n",
    "                            + predict[\"model9\"] + predict[\"model10\"] \\\n",
    "                            + predict[\"model11\"] + predict[\"model12\"] \\\n",
    "                            + predict[\"model13\"] + predict[\"model14\"] \\\n",
    "                        ) / 16.0\n",
    "\n",
    "    np.savetxt(\"output/temp.predict\", np.round(predict[\"yhatavg\"].values).astype(int), fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Global_score(y_test_lr):\n",
    "    global_predict = pd.read_csv('output/global结果的名字.predict', header=None)\n",
    "    return (MAE(np.array(global_predict), np.array(y_test_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: user_verified       $\\qquad \\qquad$ \n",
    "\n",
    "1: user_statuses_count $\\qquad \\qquad$ \n",
    "\n",
    "2: user_followers_count\n",
    "\n",
    "3: user_friends_count    \n",
    "\n",
    "4: ratio_friends_followers $\\qquad \\qquad$ \n",
    "\n",
    "5: mention_exist $\\qquad \\qquad$ \n",
    "\n",
    "6: mention_count\n",
    "\n",
    "7: url_exist $\\qquad \\qquad$ \n",
    "\n",
    "8: url_count $\\qquad \\qquad$ \n",
    "\n",
    "9: hashtag_exist\n",
    "\n",
    "10: hashtag_count $\\qquad \\qquad$ \n",
    "\n",
    "11: timeseg $\\qquad \\qquad$ \n",
    "\n",
    "12: weekend\n",
    "\n",
    "13: day_of_week$\\qquad \\qquad$\n",
    "\n",
    "14: text_length$\\qquad \\qquad$\n",
    "\n",
    "15: sentiment_pos\n",
    "\n",
    "16: sentiment_neg$\\qquad \\qquad$\n",
    "\n",
    "17: sentiment_neu$\\qquad \\qquad$\n",
    "\n",
    "18: sentiment_comp\n",
    "\n",
    "19: tf_idf_0$\\qquad \\qquad$\n",
    "\n",
    "20: tf_idf_1\n",
    "\n",
    "21: tf_idf_2\n",
    "\n",
    "22: tf_idf_3\n",
    "\n",
    "23: tf_idf_4\n",
    "\n",
    "24: tf_idf_5\n",
    "\n",
    "25: tf_idf_6\n",
    "\n",
    "26: tf_idf_7\n",
    "\n",
    "27: tf_idf_8\n",
    "\n",
    "28: tf_idf_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_set(X_train, features_number):\n",
    "    features = X_train.columns.values.tolist()\n",
    "    cols = []\n",
    "    for i in features_number:\n",
    "        cols.append(features[i])\n",
    "    return X_train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr = get_test_set(X_train, [1,2,4])\n",
    "y_train_lr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lrrf(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nnrf(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xdeepfmrf(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deepfmrf(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lr = get_test_set(X_test, [1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_lr = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrrf_predict(X_test_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnrf_predict(X_test_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmrf_predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model_predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final score\n",
    "Global_score(y_test_lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
