{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train_transformed.csv\")\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for pre-process and select features \n",
    "#features that need to be scaled\n",
    "features_need_scaled=['user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'text_length']\n",
    "# features that we select to regressor\n",
    "features_selected = ['user_verified', 'user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'mention_exist', 'url_exist','hashtag_exist',  'timeseg', 'weekend', 'day_of_week', 'text_length', 'sentiment_comp']\n",
    "\n",
    "#tuning the parameter of regressor: n_estimators and max_depth\n",
    "n_estimators=100\n",
    "max_depth = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "def cv(X,y,regressor,kf):\n",
    "    results_cv=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_tmp, X_test_tmp = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_tmp, y_test_tmp = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # normalize some features in X_train and use the same parametres to normalize these features in X_test\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_tmp[features_need_scaled])\n",
    "        X_train_tmp[features_need_scaled] = scaler.transform(X_train_tmp[features_need_scaled])\n",
    "        X_test_tmp[features_need_scaled] = scaler.transform(X_test_tmp[features_need_scaled])\n",
    "        \n",
    "        y_train_tmp=np.log(y_train_tmp+1)\n",
    "            \n",
    "        regressor.fit(X_train_tmp, y_train_tmp)\n",
    "        y_predict = regressor.predict(X_test_tmp)\n",
    "        score = mean_absolute_error(y_test_tmp, np.exp(y_predict)-1.0)\n",
    "        print('tmp score: ',score)\n",
    "        results_cv.append(score)\n",
    "    return np.mean(results_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4837            1.45m\n",
      "         2           2.1064            1.35m\n",
      "         3           1.7937            1.11m\n",
      "         4           1.5350           55.82s\n",
      "         5           1.3205           46.98s\n",
      "         6           1.1426           37.53s\n",
      "         7           0.9945           27.75s\n",
      "         8           0.8710           18.24s\n",
      "         9           0.7678            9.05s\n",
      "        10           0.6816            0.00s\n",
      "tmp score:  141.75663472848188\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4804            1.29m\n",
      "         2           2.1007            1.19m\n",
      "         3           1.7880            1.06m\n",
      "         4           1.5292           54.24s\n",
      "         5           1.3148           46.07s\n",
      "         6           1.1356           36.57s\n",
      "         7           0.9899           27.98s\n",
      "         8           0.8675           18.49s\n",
      "         9           0.7644            9.13s\n",
      "        10           0.6773            0.00s\n",
      "tmp score:  146.0306184081682\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4754            1.97m\n",
      "         2           2.0988            1.71m\n",
      "         3           1.7866            1.46m\n",
      "         4           1.5308            1.24m\n",
      "         5           1.3210            1.04m\n",
      "         6           1.1453           49.82s\n",
      "         7           0.9985           36.98s\n",
      "         8           0.8762           24.62s\n",
      "         9           0.7726           12.27s\n",
      "        10           0.6868            0.00s\n",
      "tmp score:  143.95115074616155\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4773            1.79m\n",
      "         2           2.0996            1.61m\n",
      "         3           1.7875            1.42m\n",
      "         4           1.5328            1.24m\n",
      "         5           1.3212            1.04m\n",
      "         6           1.1426           50.14s\n",
      "         7           0.9948           37.86s\n",
      "         8           0.8713           25.16s\n",
      "         9           0.7692           12.49s\n",
      "        10           0.6820            0.00s\n",
      "tmp score:  155.56833939388454\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4767            1.79m\n",
      "         2           2.0999            1.46m\n",
      "         3           1.7885            1.18m\n",
      "         4           1.5311           57.57s\n",
      "         5           1.3180           46.24s\n",
      "         6           1.1394           35.46s\n",
      "         7           0.9917           25.91s\n",
      "         8           0.8680           17.29s\n",
      "         9           0.7654            8.56s\n",
      "        10           0.6781            0.00s\n",
      "tmp score:  136.51698517896676\n",
      "Cross validation score: 144.76474569113256\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 43)\n",
    "regressor = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "\n",
    "X = train_data[features_selected]\n",
    "y = train_data['retweet_count']\n",
    "score = cv(X,y,regressor,kf)\n",
    "print('Cross validation score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict for evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           2.4848            1.88m\n",
      "         2           2.1145            1.63m\n",
      "         3           1.8076            1.46m\n",
      "         4           1.5540            1.23m\n",
      "         5           1.3439           59.09s\n",
      "         6           1.1697           46.50s\n",
      "         7           1.0244           34.10s\n",
      "         8           0.9028           22.43s\n",
      "         9           0.8004           11.09s\n",
      "        10           0.7154            0.00s\n"
     ]
    }
   ],
   "source": [
    "# use all data to train he model\n",
    "X_train= train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "X_val = eval_data[features_selected]\n",
    "\n",
    "# normalize some features in X_train and use the same parametres to normalize these features in X_eval\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train[features_need_scaled])\n",
    "X_train[features_need_scaled] = scaler.transform(X_train[features_need_scaled])\n",
    "X_val[features_need_scaled] = scaler.transform(X_val[features_need_scaled])\n",
    "\n",
    "log_gbr = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "y_train= np.log(y_train+1.)\n",
    "log_gbr.fit(X_train, y_train)\n",
    "y_pred = log_gbr.predict(X_val)\n",
    "y_pred= np.exp(y_pred)-1.0\n",
    "\n",
    "# Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"prediction/log_gbr_predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred):\n",
    "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
