{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"../data/train_transformed.csv\")\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"../data/evaluation_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter for pre-process and select features \n",
    "#features that need to be scaled\n",
    "features_need_scaled=['user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'text_length']\n",
    "# features that we select to regressor\n",
    "features_selected = ['user_verified', 'user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'mention_exist', 'url_exist','hashtag_exist',  'timeseg', 'weekend', 'day_of_week', 'text_length', 'sentiment_comp']\n",
    "\n",
    "#tuning the parameter of regressor: n_estimators and max_depth\n",
    "n_estimators=100\n",
    "max_depth = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "def cv(X,y,regressor,kf):\n",
    "    results_cv=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_tmp, X_test_tmp = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_tmp, y_test_tmp = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # normalize some features in X_train and use the same parametres to normalize these features in X_test\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_tmp[features_need_scaled])\n",
    "        X_train_tmp[features_need_scaled] = scaler.transform(X_train_tmp[features_need_scaled])\n",
    "        X_test_tmp[features_need_scaled] = scaler.transform(X_test_tmp[features_need_scaled])\n",
    "        \n",
    "            \n",
    "        regressor.fit(X_train_tmp, y_train_tmp)\n",
    "        y_predict = regressor.predict(X_test_tmp)\n",
    "        score = mean_absolute_error(y_test_tmp, y_predict)\n",
    "        print('tmp score: ',score)\n",
    "        results_cv.append(score)\n",
    "    return np.mean(results_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     8129645.7748           19.66m\n",
      "         2     6927385.5224           19.51m\n",
      "         3     5988160.1579           19.72m\n",
      "         4     5183901.6940           20.23m\n",
      "         5     4481595.0423           19.96m\n",
      "         6     3930670.7326           19.75m\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 43)\n",
    "regressor = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "\n",
    "X = train_data[features_selected]\n",
    "y = train_data['retweet_count']\n",
    "score = cv(X,y,regressor,kf)\n",
    "print('Cross validation score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7465057.4044            1.97m\n",
      "         2     6330405.2560            1.63m\n",
      "         3     5404586.9504            1.42m\n",
      "         4     4615048.3612            1.18m\n",
      "         5     4012546.4749           57.59s\n",
      "         6     3488148.5188           45.39s\n",
      "         7     3054434.7075           34.24s\n",
      "         8     2682124.0347           23.41s\n",
      "         9     2372040.4132           11.94s\n",
      "        10     2097931.6903            0.00s\n"
     ]
    }
   ],
   "source": [
    "# use all data to train he model\n",
    "X_train= train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "X_val = eval_data[features_selected]\n",
    "\n",
    "# normalize some features in X_train and use the same parametres to normalize these features in X_eval\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train[features_need_scaled])\n",
    "X_train[features_need_scaled] = scaler.transform(X_train[features_need_scaled])\n",
    "X_val[features_need_scaled] = scaler.transform(X_val[features_need_scaled])\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"../prediction/gbr_predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred):\n",
    "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
