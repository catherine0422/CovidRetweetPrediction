{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"../data/train_transformed_sample.csv\")\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"../data/evaluation_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter for pre-process and select features \n",
    "#features that need to be scaled\n",
    "features_need_scaled=['user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'text_length']\n",
    "# features that we select to regressor\n",
    "features_selected = ['user_verified', 'user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'mention_exist', 'url_exist','hashtag_exist',  'timeseg', 'weekend', 'day_of_week', 'text_length', 'sentiment_comp']\n",
    "\n",
    "#tuning the parameter of regressor: n_estimators and max_depth\n",
    "n_estimators=10\n",
    "max_depth = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "def cv(X,y,regressor,kf):\n",
    "    results_cv=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_tmp, X_test_tmp = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_tmp, y_test_tmp = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # normalize some features in X_train and use the same parametres to normalize these features in X_test\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_tmp[features_need_scaled])\n",
    "        X_train_tmp[features_need_scaled] = scaler.transform(X_train_tmp[features_need_scaled])\n",
    "        X_test_tmp[features_need_scaled] = scaler.transform(X_test_tmp[features_need_scaled])\n",
    "        \n",
    "            \n",
    "        regressor.fit(X_train_tmp, y_train_tmp)\n",
    "        y_predict = regressor.predict(X_test_tmp)\n",
    "        score = mean_absolute_error(y_test_tmp, y_predict)\n",
    "        print('tmp score: ',score)\n",
    "        results_cv.append(score)\n",
    "    return np.mean(results_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     1300997.0924            0.60s\n",
      "         2     1053839.8163            0.55s\n",
      "         3      853640.6506            0.47s\n",
      "         4      691476.9514            0.41s\n",
      "         5      560106.0803            0.34s\n",
      "         6      453693.0212            0.27s\n",
      "         7      367498.2690            0.20s\n",
      "         8      297680.3574            0.13s\n",
      "         9      241128.2727            0.07s\n",
      "        10      195319.5612            0.00s\n",
      "tmp score:  142.5046591976996\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     1375771.8882            0.77s\n",
      "         2     1114376.4257            0.66s\n",
      "         3      902646.0826            0.60s\n",
      "         4      731144.4484            0.62s\n",
      "         5      592227.8843            0.50s\n",
      "         6      479705.3253            0.38s\n",
      "         7      388562.3310            0.28s\n",
      "         8      314736.0970            0.18s\n",
      "         9      254936.7794            0.09s\n",
      "        10      206499.2944            0.00s\n",
      "tmp score:  113.68455256987328\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      476531.1720            0.60s\n",
      "         2      385993.4856            0.55s\n",
      "         3      312657.7291            0.51s\n",
      "         4      253255.6810            0.45s\n",
      "         5      205139.6188            0.37s\n",
      "         6      166165.8097            0.29s\n",
      "         7      134596.9056            0.22s\n",
      "         8      109025.9034            0.14s\n",
      "         9       88312.9909            0.07s\n",
      "        10       71535.6130            0.00s\n",
      "tmp score:  178.61551874255662\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     1413459.7889            0.57s\n",
      "         2     1144905.7089            0.55s\n",
      "         3      927376.9632            0.50s\n",
      "         4      751178.5364            0.42s\n",
      "         5      608456.3006            0.40s\n",
      "         6      492852.3475            0.31s\n",
      "         7      399211.0092            0.23s\n",
      "         8      323361.5054            0.16s\n",
      "         9      261924.3591            0.08s\n",
      "        10      212159.5010            0.00s\n",
      "tmp score:  119.96325885025753\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     1291406.1215            0.57s\n",
      "         2     1046170.1869            0.55s\n",
      "         3      847530.2423            0.49s\n",
      "         4      686627.4112            0.42s\n",
      "         5      556294.1842            0.35s\n",
      "         6      450701.3890            0.28s\n",
      "         7      365084.5786            0.21s\n",
      "         8      295732.7257            0.14s\n",
      "         9      239552.5024            0.07s\n",
      "        10      194045.5375            0.00s\n",
      "tmp score:  226.74606284402597\n",
      "Cross validation score: 156.3028104408826\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 43)\n",
    "regressor = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "\n",
    "X = train_data[features_selected]\n",
    "y = train_data['retweet_count']\n",
    "score = cv(X,y,regressor,kf)\n",
    "print('Cross validation score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     1171629.0524            0.79s\n",
      "         2      949028.5435            0.73s\n",
      "         3      768721.6367            0.68s\n",
      "         4      622672.5273            0.56s\n",
      "         5      504372.2723            0.48s\n",
      "         6      408547.9873            0.38s\n",
      "         7      330929.9869            0.28s\n",
      "         8      268059.8849            0.19s\n",
      "         9      217133.4453            0.09s\n",
      "        10      175883.7233            0.00s\n"
     ]
    }
   ],
   "source": [
    "# use all data to train he model\n",
    "X_train= train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "X_val = eval_data[features_selected]\n",
    "\n",
    "# normalize some features in X_train and use the same parametres to normalize these features in X_eval\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train[features_need_scaled])\n",
    "X_train[features_need_scaled] = scaler.transform(X_train[features_need_scaled])\n",
    "X_val[features_need_scaled] = scaler.transform(X_val[features_need_scaled])\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"../prediction/gbr_predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred):\n",
    "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
