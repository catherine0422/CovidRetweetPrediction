{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"../data/train_transformed.csv\")\n",
    "#load the evaluation data\n",
    "eva_data = pd.read_csv(\"../data/evaluation_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter for pre-process and select features \n",
    "#features that need to be scaled\n",
    "features_need_scaled=['user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'text_length']\n",
    "# features that we select to regressor\n",
    "features_selected = ['user_verified', 'user_statuses_count', 'user_followers_count', \n",
    "                     'user_friends_count', 'ratio_friends_followers', 'mention_exist',\n",
    "                     'url_exist','hashtag_exist',  'timeseg', 'weekend', 'day_of_week',\n",
    "                     'text_length', 'sentiment_comp']\n",
    "\n",
    "#tuning the parameter of regressor: n_estimators and max_depth\n",
    "n_estimators = 100\n",
    "max_depth = 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model enhanced that we define\n",
    "def svr_rf(X_train, y_train, max_depth, n_estimators):\n",
    "    # svr\n",
    "    svr = LinearSVR()\n",
    "    y_train_log = np.log(y_train+1)\n",
    "    svr.fit(X_train, y_train_log) \n",
    "    svr_y_train_predict = svr.predict(X_train) # this produces a ndarray\n",
    "    \n",
    "    # random forest\n",
    "    # for training residual RF\n",
    "    rf_y_train = y_train_log - svr_y_train_predict\n",
    "    \n",
    "    reg = RandomForestRegressor(max_depth = max_depth,   \n",
    "                                n_estimators = n_estimators, \n",
    "                                random_state = 7,  \n",
    "                                n_jobs = 10, \n",
    "                                verbose = 5)  \n",
    "\n",
    "    reg.fit(X_train, rf_y_train)\n",
    "    return svr, reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Corss-validation for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10building tree 2 of 10\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10\n",
      "\n",
      "building tree 7 of 10building tree 8 of 10\n",
      "building tree 9 of 10\n",
      "building tree 10 of 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   13.6s remaining:   31.8s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   14.6s remaining:    9.7s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.13848965611126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 10building tree 2 of 10\n",
      "\n",
      "building tree 3 of 10\n",
      "building tree 4 of 10\n",
      "building tree 5 of 10\n",
      "building tree 6 of 10building tree 7 of 10\n",
      "\n",
      "building tree 8 of 10\n",
      "building tree 9 of 10building tree 10 of 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   14.4s remaining:   33.7s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:   14.4s remaining:    9.6s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   14.5s finished\n",
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=10)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.1738258097865\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 43)\n",
    "results_cv=[]\n",
    "\n",
    "for train_index, test_index in kf.split(np.array(X_train)):\n",
    "    X_train_tmp, X_test_tmp = pd.DataFrame(np.array(X_train)[train_index]), pd.DataFrame(np.array(X_train)[test_index])\n",
    "    y_train_tmp, y_test_tmp = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    \n",
    "    #  normalize some features in X_train and use the same parametres to normalize these features in X_test\n",
    "    X_train_tmp.columns = features_selected\n",
    "    X_test_tmp.columns = features_selected\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(X_train_tmp[features_need_scaled])\n",
    "    X_train_tmp[features_need_scaled] = scaler.transform(X_train_tmp[features_need_scaled])\n",
    "    X_test_tmp[features_need_scaled] = scaler.transform(X_test_tmp[features_need_scaled])\n",
    "    \n",
    "    # train\n",
    "    svr, reg = svr_rf(X_train_tmp, y_train_tmp, max_depth, n_estimators)\n",
    "    \n",
    "    # predict\n",
    "    svr_predict_y_log = svr.predict(X_test_tmp)\n",
    "\n",
    "    rf_predict_y_log = reg.predict(X_test_tmp)\n",
    "\n",
    "    y_predict = np.exp(svr_predict_y_log + rf_predict_y_log) -1\n",
    "\n",
    "    print(mean_absolute_error(y_test_tmp, y_predict))\n",
    "    results_cv.append(mean_absolute_error(y_test_tmp, y_predict))\n",
    "\n",
    "score=np.array(results_cv).mean()\n",
    "print('Cross validation score:',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use all data to train he model\n",
    "X_train = train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "X_val = eva_data[features_selected]\n",
    "\n",
    "# normalize some features in X_train and use the same parametres to normalize these features in X_eval\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train[features_need_scaled])\n",
    "X_train[features_need_scaled] = scaler.transform(X_train[features_need_scaled])\n",
    "X_val[features_need_scaled] = scaler.transform(X_val[features_need_scaled])\n",
    "\n",
    "svr2, reg2 = svr_rf(X_train, y_train, max_depth, n_estimators)\n",
    "svr_predict_y_log_eva = svr2.predict(X_val)\n",
    "\n",
    "rf_predict_y_log_eva = reg2.predict(X_val)\n",
    "\n",
    "y_predict_eva = np.exp(svr_predict_y_log_eva + rf_predict_y_log_eva) - 1\n",
    "\n",
    "#Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"../prediction/svr_rf_predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_predict_eva):\n",
    "        writer.writerow([str(eva_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
