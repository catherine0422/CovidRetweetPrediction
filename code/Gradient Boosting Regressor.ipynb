{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train_transformed.csv\")\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter for pre-process and select features \n",
    "#features that need to be scaled\n",
    "features_need_scaled=['user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'text_length']\n",
    "# features that we select to regressor\n",
    "features_selected = ['user_verified', 'user_statuses_count', 'user_followers_count', 'user_friends_count', 'ratio_friends_followers', 'mention_exist', 'url_exist','hashtag_exist',  'timeseg', 'weekend', 'day_of_week', 'text_length', 'sentiment_comp']\n",
    "\n",
    "#tuning the parameter of regressor: n_estimators and max_depth\n",
    "n_estimators=100\n",
    "max_depth = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-validation for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation\n",
    "def cv(X,y,regressor,kf):\n",
    "    results_cv=[]\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train_tmp, X_test_tmp = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        y_train_tmp, y_test_tmp = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # normalize some features in X_train and use the same parametres to normalize these features in X_test\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler.fit(X_train_tmp[features_need_scaled])\n",
    "        X_train_tmp[features_need_scaled] = scaler.transform(X_train_tmp[features_need_scaled])\n",
    "        X_test_tmp[features_need_scaled] = scaler.transform(X_test_tmp[features_need_scaled])\n",
    "        \n",
    "            \n",
    "        regressor.fit(X_train_tmp, y_train_tmp)\n",
    "        y_predict = regressor.predict(X_test_tmp)\n",
    "        score = mean_absolute_error(y_test_tmp, y_predict)\n",
    "        print('tmp score: ',score)\n",
    "        results_cv.append(score)\n",
    "    return np.mean(results_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     8129645.7748            1.85m\n",
      "         2     6927385.5224            1.64m\n",
      "         3     5988160.1579            1.45m\n",
      "         4     5183901.6940            1.24m\n",
      "         5     4481595.0423            1.04m\n",
      "         6     3930670.7326           50.18s\n",
      "         7     3473746.0407           37.23s\n",
      "         8     3045697.3532           23.63s\n",
      "         9     2666709.1422           11.54s\n",
      "        10     2368975.6743            0.00s\n",
      "tmp score:  238.95699940076253\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7388760.6813            1.55m\n",
      "         2     6220304.4029            1.45m\n",
      "         3     5276560.7967            1.20m\n",
      "         4     4497525.9060            1.02m\n",
      "         5     3859529.4915           50.95s\n",
      "         6     3336375.6391           40.62s\n",
      "         7     2867489.3136           29.91s\n",
      "         8     2477245.2642           19.69s\n",
      "         9     2186768.7244            9.79s\n",
      "        10     1915745.2728            0.00s\n",
      "tmp score:  241.79727872509588\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     6893800.9828            1.49m\n",
      "         2     5833898.0358            1.35m\n",
      "         3     4991627.9865            1.16m\n",
      "         4     4296157.2614            1.01m\n",
      "         5     3753196.3676           50.74s\n",
      "         6     3313126.9722           41.27s\n",
      "         7     2893125.5050           30.26s\n",
      "         8     2501097.4582           20.13s\n",
      "         9     2189369.2022           10.56s\n",
      "        10     1907521.3791            0.00s\n",
      "tmp score:  237.3866258173045\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     6820736.1936            1.98m\n",
      "         2     5721922.3736            1.71m\n",
      "         3     4841598.5645            1.53m\n",
      "         4     4098498.8468            1.31m\n",
      "         5     3479201.4381            1.09m\n",
      "         6     2976578.9899           52.36s\n",
      "         7     2547704.4432           39.14s\n",
      "         8     2205235.0068           26.03s\n",
      "         9     1895682.4553           13.02s\n",
      "        10     1655883.3567            0.00s\n",
      "tmp score:  253.63178361915016\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1     8182544.5773            2.08m\n",
      "         2     6963006.1037            1.87m\n",
      "         3     5954255.5982            1.62m\n",
      "         4     5138129.7155            1.39m\n",
      "         5     4448602.1785            1.16m\n",
      "         6     3871060.3786           54.66s\n",
      "         7     3391526.8457           40.48s\n",
      "         8     3011257.2129           26.84s\n",
      "         9     2644832.4344           13.19s\n",
      "        10     2336524.9994            0.00s\n",
      "tmp score:  232.71157769558147\n",
      "Cross validation score: 240.89685305157892\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 43)\n",
    "regressor = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "\n",
    "X = train_data[features_selected]\n",
    "y = train_data['retweet_count']\n",
    "score = cv(X,y,regressor,kf)\n",
    "print('Cross validation score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1     7465057.4044            1.97m\n",
      "         2     6330405.2560            1.63m\n",
      "         3     5404586.9504            1.42m\n",
      "         4     4615048.3612            1.18m\n",
      "         5     4012546.4749           57.59s\n",
      "         6     3488148.5188           45.39s\n",
      "         7     3054434.7075           34.24s\n",
      "         8     2682124.0347           23.41s\n",
      "         9     2372040.4132           11.94s\n",
      "        10     2097931.6903            0.00s\n"
     ]
    }
   ],
   "source": [
    "# use all data to train he model\n",
    "X_train= train_data[features_selected]\n",
    "y_train = train_data['retweet_count']\n",
    "X_val = eval_data[features_selected]\n",
    "\n",
    "# normalize some features in X_train and use the same parametres to normalize these features in X_eval\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X_train[features_need_scaled])\n",
    "X_train[features_need_scaled] = scaler.transform(X_train[features_need_scaled])\n",
    "X_val[features_need_scaled] = scaler.transform(X_val[features_need_scaled])\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=n_estimators,\n",
    "                                verbose=5,\n",
    "                            max_depth = max_depth,\n",
    "                            random_state =12)# we tuning the parameter here n_estimators and max_depth\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "y_pred = gbr.predict(X_val)\n",
    "\n",
    "# Dump the results into a file that follows the required Kaggle template\n",
    "with open(\"prediction/gbr_predictions.csv\", 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"TweetID\", \"NoRetweets\"])\n",
    "    for index, prediction in enumerate(y_pred):\n",
    "        writer.writerow([str(eval_data['id'].iloc[index]) , str(int(prediction))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
